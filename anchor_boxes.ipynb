{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khaledlarbi/MVA_DL_TrashDetection/blob/main/anchor_boxes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc": true,
        "id": "6XSixqNNb_7Z"
      },
      "source": [
        "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
        "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Preparation-of-the-proposed-region-to-train-the-Fast-RCNN\" data-toc-modified-id=\"Preparation-of-the-proposed-region-to-train-the-Fast-RCNN-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Preparation of the proposed region to train the Fast RCNN</a></span></li><li><span><a href=\"#Region-proposal-network\" data-toc-modified-id=\"Region-proposal-network-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Region proposal network</a></span></li><li><span><a href=\"#Training-RPN-network\" data-toc-modified-id=\"Training-RPN-network-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Training RPN network</a></span></li><li><span><a href=\"#Coco-dataset\" data-toc-modified-id=\"Coco-dataset-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Coco dataset</a></span></li></ul></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gZieto6b_7d"
      },
      "source": [
        "This notebook aims to provide functions that produce anchor boxes as decribed in the paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YTdXFc3b_7e"
      },
      "source": [
        "A box will be describe either as a numpy array $[y^-, x^-, y^+, x^+]$  or as a numpy array $[c_y, c_x, h,w]$\n",
        "\n",
        "TODO : CHECK +1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Anchors"
      ],
      "metadata": {
        "id": "gEMPqMqMrBsn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:00.279181Z",
          "start_time": "2021-12-08T23:11:58.115463Z"
        },
        "id": "R6wLPDnOb_7e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches #In order to draw the box ! (je sais pas pourquoi j'écris en anglais)\n",
        "from torchvision import models\n",
        "import torch.utils.data as data\n",
        "from PIL import Image\n",
        "import os\n",
        "import os.path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:00.294254Z",
          "start_time": "2021-12-08T23:12:00.281183Z"
        },
        "id": "RtROAiS1b_7f"
      },
      "outputs": [],
      "source": [
        "def vertice_to_yxhw(anchor):\n",
        "    res = (np.mean((anchor[0],anchor[2])),np.mean((anchor[1],anchor[3])), anchor[2] - anchor[0] + 1, anchor[3] - anchor[1]+1)\n",
        "    return np.array(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:00.310283Z",
          "start_time": "2021-12-08T23:12:00.296255Z"
        },
        "id": "nzn9r1Dzb_7g"
      },
      "outputs": [],
      "source": [
        "def yxhw_to_vertice(anchor):\n",
        "    res = (anchor[0] - anchor[2]/2, anchor[1] - anchor[3]/2, anchor[0] + anchor[2]/2, anchor[1] + anchor[3]/2)\n",
        "    return np.array(res)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def xywh_to_vertice(anchor):\n",
        "  anchor_perm = (anchor[1] + 0.5*anchor[3] ,anchor[0] + 0.5*anchor[2],anchor[3],anchor[2])\n",
        "  return yxhw_to_vertice(anchor_perm)"
      ],
      "metadata": {
        "id": "4DmF9weOH8Uj"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:00.326279Z",
          "start_time": "2021-12-08T23:12:00.312275Z"
        },
        "id": "4NrJLfwrb_7g"
      },
      "outputs": [],
      "source": [
        "def anchor_box(center, ratio, scale, shape_initial, shape_featured):\n",
        "    sub_width = shape_initial[0]/shape_featured[0]\n",
        "    sub_height = shape_initial[1]/shape_featured[1]\n",
        "    anchor_width = sub_width*scale*np.sqrt(ratio)\n",
        "    anchor_height = sub_height*scale/np.sqrt(ratio)\n",
        "    \n",
        "    ym = center[1] - anchor_height/2\n",
        "    yp = center[1] + anchor_height/2\n",
        "    xm = center[0] - anchor_width/2\n",
        "    xp = center[0] + anchor_width/2\n",
        "    \n",
        "    anchor = np.array((ym,xm,yp,xp))\n",
        "    return(anchor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:00.341279Z",
          "start_time": "2021-12-08T23:12:00.328260Z"
        },
        "id": "MEYHa3sPb_7h"
      },
      "outputs": [],
      "source": [
        "def list_centers(shape_initial, shape_featured):\n",
        "    ratio_h = shape_initial[1]/shape_featured[1]\n",
        "    ratio_w = shape_initial[0]/shape_featured[0]\n",
        "    #intiail center is the center at the left top corner\n",
        "    all_centers = [np.array((ratio_w/2, ratio_h/2),dtype=float) + np.array((ratio_w*i, ratio_h*j),dtype=float) for i in range(int(shape_featured[0])) for j in range(int(shape_featured[1]))]\n",
        "    return(all_centers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:00.356279Z",
          "start_time": "2021-12-08T23:12:00.343272Z"
        },
        "id": "2klfx8Fjb_7h"
      },
      "outputs": [],
      "source": [
        "def anchor_boxes(list_ratios, list_scales, shape_initial, shape_featured):\n",
        "    list_center = list_centers(shape_initial, shape_featured)\n",
        "    all_anchors = [anchor_box(center, ratio, scale,shape_initial,shape_featured) for center in list_center for ratio in list_ratios\n",
        "                   for scale in list_scales]\n",
        "    return(all_anchors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:00.372270Z",
          "start_time": "2021-12-08T23:12:00.358270Z"
        },
        "id": "Y1jD1fLqb_7i"
      },
      "outputs": [],
      "source": [
        "def check_anchor_inside(anchor_box, shape_initial):\n",
        "    ym = anchor_box[0]\n",
        "    yp = anchor_box[2]\n",
        "    xm = anchor_box[1]\n",
        "    xp = anchor_box[3]\n",
        "    is_inside = (min(xm,xp)>0) & (max(xm,xp)<shape_initial[0]) & (max(yp,ym) < shape_initial[1]) & (min(ym,yp) > 0) \n",
        "    return(is_inside)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:00.387270Z",
          "start_time": "2021-12-08T23:12:00.374270Z"
        },
        "id": "YE4ek9ZYb_7i"
      },
      "outputs": [],
      "source": [
        "def iou(box1,box2):\n",
        "    xm = max(box1[1], box2[1])\n",
        "    xp = min(box1[3], box2[3])\n",
        "    ym = max(box1[0], box2[0])\n",
        "    yp = min(box1[2], box2[2])\n",
        "    \n",
        "    intersection = 0\n",
        "    \n",
        "    if((xm < xp) &(ym < yp)):\n",
        "        intersection = (xp - xm)*(yp-ym)\n",
        "    \n",
        "    union = (box1[3]-box1[1])*(box1[2] - box1[0]) + (box2[3]-box2[1])*(box2[2] - box2[0]) - intersection\n",
        "    return(intersection/union)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:00.403279Z",
          "start_time": "2021-12-08T23:12:00.391276Z"
        },
        "id": "bIDDndBMb_7i"
      },
      "outputs": [],
      "source": [
        "def iou_anchors_vs_gtbox(list_anchors, list_gt_box):\n",
        "    res = np.transpose([[iou(anchor, gt_box) for anchor in list_anchors] for gt_box in list_gt_box])\n",
        "    return(np.array(res))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTlLHh_Vb_7j"
      },
      "source": [
        "**TODO** : changer la forme de cette fonction en utilisant que des *arrays*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:00.419270Z",
          "start_time": "2021-12-08T23:12:00.406270Z"
        },
        "id": "B8S6FE2db_7j"
      },
      "outputs": [],
      "source": [
        "#Return an array with :\n",
        "#for all ground truth box, the anchors which maximize the IOU with it\n",
        "#for all anchor, the max of the IOU\n",
        "\n",
        "#the first column of the array is the index and the last the IOU \n",
        "def best_anchors_from_iou(dt_anchors_vs_gtbox):\n",
        "    #index highest by gtbox (cond a)\n",
        "    dt_anchors_vs_gtbox.argmax(axis = 0)\n",
        "    ind_argmax = np.where(dt_anchors_vs_gtbox == dt_anchors_vs_gtbox.max(axis = 0))[0]\n",
        "    cond_a = dt_anchors_vs_gtbox[ind_argmax,:].max(axis = 1)\n",
        "    \n",
        "    #highest by anchors box (cond b)\n",
        "    index = dt_anchors_vs_gtbox.argmax(axis = 1)\n",
        "    iou_max = dt_anchors_vs_gtbox.max(axis = 1)\n",
        "    cond_b = dt_anchors_vs_gtbox[np.arange(dt_anchors_vs_gtbox.shape[0]),index]\n",
        "    \n",
        "    index_res = np.concatenate((ind_argmax,np.arange(dt_anchors_vs_gtbox.shape[0])))\n",
        "    res = np.concatenate((cond_a, cond_b), axis=0)\n",
        "    res = np.column_stack((index_res,res))\n",
        "    return(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:00.434260Z",
          "start_time": "2021-12-08T23:12:00.421270Z"
        },
        "id": "9VqTF3Itb_7j"
      },
      "outputs": [],
      "source": [
        "#label_from_iou returns a np.array containing for each anchor its label. (+1 if foreground, 0 if background and -1 if not used\n",
        "#during the learning phase)\n",
        "#The default thresholds are defined according the original paper about Fatest RCNN.\n",
        "\n",
        "def label_from_iou(dt_anchors_vs_gtbox,pos_threshold = 0.7, neg_threshold = 0.3):\n",
        "    label = np.full(dt_anchors_vs_gtbox.shape[0],-1)\n",
        "    iou_max = dt_anchors_vs_gtbox.max(axis = 1)\n",
        "    #positive labels : 1\n",
        "    label[iou_max > pos_threshold] = 1\n",
        "    #negative labels : 0\n",
        "    label[iou_max < neg_threshold] = 0\n",
        "    sum((iou_max < neg_threshold))\n",
        "\n",
        "    #for anchors whose maximize IOU for a given object : +1\n",
        "    dt_anchors_vs_gtbox.argmax(axis = 0)\n",
        "    ind_argmax = np.where(dt_anchors_vs_gtbox == dt_anchors_vs_gtbox.max(axis = 0))[0]\n",
        "    label[ind_argmax] = 1\n",
        "    return(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:00.449260Z",
          "start_time": "2021-12-08T23:12:00.436260Z"
        },
        "id": "aV_HYx5Ab_7k"
      },
      "outputs": [],
      "source": [
        "def loc(anchor_box, gt_box):\n",
        "    anchor_box = vertice_to_yxhw(anchor_box)\n",
        "    gt_box = vertice_to_yxhw(gt_box)\n",
        "    \n",
        "    y = (gt_box[0] - anchor_box[0])/anchor_box[2]\n",
        "    x = (gt_box[1] - anchor_box[1])/anchor_box[3]\n",
        "    w = np.log(gt_box[3]/anchor_box[3])\n",
        "    h = np.log(gt_box[2]/anchor_box[2])\n",
        "    \n",
        "    return np.array((y,x,h,w))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:00.465251Z",
          "start_time": "2021-12-08T23:12:00.451274Z"
        },
        "id": "CI9iedq9b_7k"
      },
      "outputs": [],
      "source": [
        "def deloc(anchor_box, reparam_box):\n",
        "    anchor_box = vertice_to_yxhw(anchor_box)\n",
        "    y = anchor_box[0] + (reparam_box[0] * anchor_box[2])\n",
        "    x = anchor_box[1] + (reparam_box[1] * anchor_box[3])\n",
        "    h = np.exp(reparam_box[2])*anchor_box[2]\n",
        "    w = np.exp(reparam_box[3])*anchor_box[3]\n",
        "    return np.array((y,x,h,w))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:00.481256Z",
          "start_time": "2021-12-08T23:12:00.467253Z"
        },
        "id": "jeMfYSLTb_7k"
      },
      "outputs": [],
      "source": [
        "def reparam_all_anchors(list_anchors, list_gt_box,iou,pos_threshold = 0.7, neg_threshold = 0.3):\n",
        "    index_max_gtbox = iou.argmax(axis = 1)\n",
        "    gt_box_by_anchors = [list_gt_box[i] for i in index_max_gtbox]\n",
        "    #TODO : change suboptimal ZIP\n",
        "    res = [loc(anchor, gt_box) for anchor,gt_box in zip(list_anchors, gt_box_by_anchors)] \n",
        "    #compute labels\n",
        "    labels = label_from_iou(iou, pos_threshold, neg_threshold)\n",
        "    return res,labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:00.497279Z",
          "start_time": "2021-12-08T23:12:00.483259Z"
        },
        "id": "FSjeRrgIb_7k"
      },
      "outputs": [],
      "source": [
        "def deparam_all_anchors(list_anchors, list_box_param):\n",
        "    res = [(deloc(anchor, box_param)) for anchor,box_param in zip(list_anchors, list_box_param)]\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:00.513279Z",
          "start_time": "2021-12-08T23:12:00.500252Z"
        },
        "id": "sORoyapOb_7l"
      },
      "outputs": [],
      "source": [
        "#TODO : heck how to fill when\n",
        "\n",
        "def index_training_proposal(dt_anchors_vs_gtbox, nsize = 256, pos_ratio = 0.5,pos_threshold = 0.7, neg_threshold = 0.3):\n",
        "    #number of positive units we need to reach in the training sample (we want a balanced sample)\n",
        "    nb_pos_to_drawn = round(nsize*pos_ratio)\n",
        "    lab = label_from_iou(dt_anchors_vs_gtbox, pos_threshold, neg_threshold)\n",
        "    pos_lab = np.where(lab == 1)[0]\n",
        "    neg_lab = np.where(lab == 0)[0]\n",
        "    pos = len(pos_lab)\n",
        "    neg = len(neg_lab)\n",
        "    \n",
        "    if (pos > nb_pos_to_drawn):\n",
        "        disabled_index_pos = np.random.choice(pos_lab, size=(pos - nb_pos_to_drawn), replace = False)\n",
        "        lab[disabled_index_pos] = -1\n",
        "    \n",
        "    if (neg > nsize - nb_pos_to_drawn):\n",
        "        if(pos < nb_pos_to_drawn):\n",
        "            disabled_index_neg = np.random.choice(neg_lab, size=(neg - nsize + pos), replace = False)\n",
        "        else:\n",
        "            disabled_index_neg = np.random.choice(neg_lab, size=(neg + nb_pos_to_drawn - nsize), replace = False)\n",
        "        \n",
        "        lab[disabled_index_neg] = -1    \n",
        "    \n",
        "    res = np.where((lab == 0) | (lab == 1))[0]\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:00.529260Z",
          "start_time": "2021-12-08T23:12:00.515270Z"
        },
        "id": "3VXA4Re5b_7l"
      },
      "outputs": [],
      "source": [
        "#TODO : heck how to fill when\n",
        "\n",
        "def batch_training_proposal_RPN(image, feature_shape, ratios, scales,gt_box,nsize = 256, pos_ratio = 0.5, pos_threshold = 0.7, neg_threshold = 0.3):\n",
        "    #define all anchors using the feature map and the initial picture shapes.\n",
        "    anchors_boxes = anchor_boxes(ratios, scales, tuple(image.shape[2:]), tuple(feature_shape[2:]))\n",
        "    #check if each box is inside the initial image\n",
        "    index_boxes_inside = [j for j in range(len(anchors_boxes)) if check_anchor_inside(anchors_boxes[j], image_torch.shape[2:])]\n",
        "    anchors_boxes = [anchors_boxes[j] for j in index_boxes_inside]\n",
        "    #IOU anchors vs gt box\n",
        "    iou_anc_gt_box = iou_anchors_vs_gtbox(anchors_boxes, gt_box)\n",
        "    #Index of the units we keep\n",
        "    ind_for_sample = index_training_proposal(iou_anc_gt_box,nsize, pos_ratio)\n",
        "    anchors_boxes_reparam,lab_anchors = reparam_all_anchors(anchors_boxes,gt_box,iou_anc_gt_box,pos_threshold, neg_threshold)\n",
        "    return ({\"image\" : image, \"boxes\" : torch.from_numpy(np.array(anchors_boxes_reparam)[ind_for_sample,:]),\n",
        "             \"labels\" : torch.from_numpy(lab_anchors[ind_for_sample]), \"indices\" : np.array(index_boxes_inside)[ind_for_sample]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:00.559249Z",
          "start_time": "2021-12-08T23:12:00.531283Z"
        },
        "id": "rg80p3vKb_7l"
      },
      "outputs": [],
      "source": [
        "image_torch = 800*torch.rand((1,3,800,800))\n",
        "feature_torch = torch.rand([1,512,50,50])\n",
        "\n",
        "ratio = [0.5, 1, 2]\n",
        "anchor_scales = [8, 16, 32]\n",
        "\n",
        "gt_box = [np.array([20, 30, 400, 500]), np.array([300, 400, 500, 600])]\n",
        "labels_gt_box = np.array((\"chien\",\"chat\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:01.735330Z",
          "start_time": "2021-12-08T23:12:00.561252Z"
        },
        "id": "vMpuxd_5b_7l"
      },
      "outputs": [],
      "source": [
        "batch_rpn = batch_training_proposal_RPN(image_torch, feature_torch, ratio, anchor_scales, gt_box,256,0.5,0.7,0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkGNzP5pb_7m"
      },
      "source": [
        "# Preparation of the proposed region to train the Fast RCNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:01.750681Z",
          "start_time": "2021-12-08T23:12:01.737311Z"
        },
        "id": "EJeoQrA2b_7m"
      },
      "outputs": [],
      "source": [
        "def clip_predicted_boxes(list_box, th_min, th_max):\n",
        "    list_box = np.array(list_box)\n",
        "    return list(np.clip(list_box,th_min,th_max))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:01.766679Z",
          "start_time": "2021-12-08T23:12:01.752679Z"
        },
        "id": "sX5nhvlVb_7m"
      },
      "outputs": [],
      "source": [
        "#remove all boxes with at least the width or the height less that 16\n",
        "def boxes_hw_min(list_box, list_score, min_size = 16):\n",
        "    boxes = np.array(list_box)\n",
        "    height = boxes[:, 2] - boxes[:, 0]\n",
        "    width = boxes[:, 3] - boxes[:, 1]\n",
        "    box_kept = np.where((height > min_size) & (width > min_size))[0]\n",
        "    list_box_kept = [list_box[j] for j in box_kept]\n",
        "    list_score = [list_score[j] for j in box_kept]\n",
        "    return list_box_kept, list_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:01.782680Z",
          "start_time": "2021-12-08T23:12:01.768698Z"
        },
        "id": "l4n3c2yab_7m"
      },
      "outputs": [],
      "source": [
        "def nms(list_box, list_score, top_pre, top_post, thresold):\n",
        "    list_score = np.array(list_score)\n",
        "    order = list_score.argsort()[::-1]\n",
        "    order = order[:top_pre]\n",
        "    keep = []\n",
        "    list_box = np.array(list_box)\n",
        "    \n",
        "    ym = list_box[:,0]\n",
        "    xm = list_box[:,1]\n",
        "    yp = list_box[:,2]\n",
        "    xp = list_box[:,3]\n",
        "    areas = (xp - xm + 1) * (yp - ym + 1)\n",
        "\n",
        "    while len(order)>0:\n",
        "        i = order[0]\n",
        "        yym = np.maximum(ym[i], ym[order[1:]])\n",
        "        xxm = np.maximum(xm[i], xm[order[1:]])\n",
        "        yyp = np.minimum(yp[i], yp[order[1:]])\n",
        "        xxp = np.minimum(xp[i], xp[order[1:]])\n",
        "        \n",
        "        width = np.maximum(0.0, xxp - xxm + 1)\n",
        "        height = np.maximum(0.0, yyp - yym + 1)\n",
        "        intersection = width*height\n",
        "        ovr = intersection/(areas[i] + areas[order[1:]] - intersection)\n",
        "        \n",
        "        ind_to_keep = np.where(ovr <= thresold)[0]\n",
        "        order = order[ind_to_keep + 1]\n",
        "        keep.append(i)\n",
        "    \n",
        "    keep = keep[:top_post]\n",
        "    return(list_box[keep,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp5YhD2Qb_7m"
      },
      "source": [
        "# Region proposal network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:44.975470Z",
          "start_time": "2021-12-08T23:12:44.966089Z"
        },
        "id": "9zJP45j1b_7m"
      },
      "outputs": [],
      "source": [
        "#boxes as tensor [N, 5]\n",
        "def roi_pooling(boxes, feature_map,scale,adaptative_max_pool):\n",
        "    boxes_coord = boxes[:,1:].mul(scale).long() #scale + round\n",
        "    res = [feature_map.narrow(0, boxes[i,0].int(),1)[..., boxes_coord[i,1]:(boxes_coord[i,3]+1), boxes_coord[i,0]:(boxes_coord[i,2]+1)] for i in range(boxes_coord.shape[0])]\n",
        "    res = [adaptative_max_pool(element) for element in res]\n",
        "    res = torch.cat(res, axis = 0)\n",
        "    return(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:53.742770Z",
          "start_time": "2021-12-08T23:12:53.724769Z"
        },
        "id": "kwKmwLDsb_7m"
      },
      "outputs": [],
      "source": [
        "#0 in labels_gt_box must be the background\n",
        "def batch_training_proposal_FastRCNN(feature_map,list_box,list_gt_box,labels_gt_box, nsize = 128, pos_ratio = 0.25, pos_iou_threshold = 0.5,\n",
        "                                    neg_iou_threshold_p = 0.5, neg_iou_threshold_n = 0.0, adaptative_max_pool = torch.nn.AdaptiveMaxPool2d((7,7),return_indices=False),scale = 1):\n",
        "    #compute iou between each pair\n",
        "    dt_anchors_vs_gtbox = iou_anchors_vs_gtbox(list_box,list_gt_box)\n",
        "    \n",
        "    #number of positive units we need to reach in the training sample (we want a balanced sample)\n",
        "    nb_pos_to_drawn = round(nsize*pos_ratio)\n",
        "    iou = iou_anchors_vs_gtbox(roi_pred, gt_box)\n",
        "    #compute the maximum for each anchor\n",
        "    gt_roi_label = np.argmax(iou, axis = 1)\n",
        "    gt_roi_max = np.max(iou, axis = 1)\n",
        "    labels = labels_gt_box[gt_roi_label]\n",
        "    \n",
        "    #assign the label if greater that pos_iou_threshold\n",
        "    #assign background if between the two negative thresholds\n",
        "    gt_pos = np.where((gt_roi_max > pos_iou_threshold))[0]\n",
        "    gt_neg = np.where((gt_roi_max < neg_iou_threshold_p) & (gt_roi_max > neg_iou_threshold_n))[0] #background -- 0\n",
        "\n",
        "    #Nb of positives and negatives boxes get using the thresholds\n",
        "    pos = len(gt_pos)\n",
        "    neg = len(gt_neg)\n",
        "    \n",
        "    #Subsampling from it\n",
        "    if (pos > nb_pos_to_drawn):\n",
        "        disabled_index_pos = np.random.choice(range(len(gt_pos)), size=(pos - nb_pos_to_drawn), replace = False)\n",
        "        gt_pos = np.delete(gt_pos, disabled_index_pos)\n",
        "    \n",
        "    if (neg > nsize - nb_pos_to_drawn):\n",
        "        if(pos < nb_pos_to_drawn):\n",
        "            disabled_index_neg = np.random.choice(range(len(gt_neg)), size=(neg - nsize + pos), replace = False)\n",
        "            gt_neg = np.delete(gt_neg, disabled_index_neg)\n",
        "        else:\n",
        "            disabled_index_neg = np.random.choice(range(len(gt_neg)), size=(neg + nb_pos_to_drawn - nsize), replace = False)\n",
        "            gt_neg = np.delete(gt_neg, disabled_index_neg)\n",
        "        \n",
        "    \n",
        "    #if negative : assign background labels with it's \"0\"\n",
        "    labels[gt_neg] = \"0\"\n",
        "    final_index = np.append(gt_pos,gt_neg)\n",
        "    \n",
        "    #Non reparams\n",
        "    non_reparam = np.array(list_box)[final_index,:]\n",
        "    #Need to transform from yxhw to xywh\n",
        "    non_reparam = non_reparam[:,(1,0,3,2)]\n",
        "    non_reparam = np.hstack((np.zeros((non_reparam.shape[0],1)), non_reparam))\n",
        "    non_reparam = torch.from_numpy(non_reparam)\n",
        "    data_for_training =roi_pooling(non_reparam, feature_map,scale, adaptive_max_pool)\n",
        "    #Reparams\n",
        "    reparam = [loc(box,gt_box) for box,gt_box in zip(non_reparam, list(np.array(list_gt_box)[gt_roi_label,:]))]\n",
        "    \n",
        "    return  data_for_training, reparam,labels[final_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:13:26.269414Z",
          "start_time": "2021-12-08T23:13:26.260394Z"
        },
        "id": "5hiMhRkjb_7n"
      },
      "outputs": [],
      "source": [
        "#data_x, box_y, label_y = batch_training_proposal_FastRCNN(test,roi_pred,gt_box,labels_gt_box, nsize = 128, pos_ratio = 0.25, pos_iou_threshold = 0.5,\n",
        "#                                    neg_iou_threshold_p = 0.5, neg_iou_threshold_n = 0.0,adaptative_max_pool = torch.nn.AdaptiveMaxPool2d((7,7),return_indices=False),\n",
        "#                                 scale = 1/16.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEAZuBOiX_CF"
      },
      "source": [
        "# Load train dataset \n",
        "\n",
        "J'ai réussi à utiliser l'API Coco via `torchvision.datasets.CocoDetection` (https://pytorch.org/vision/stable/datasets.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kc-SDR8e-P0z"
      },
      "source": [
        "#Permet d'utiliser la co des serveurs Google (rip la mienne) et assure une meilleure reproductibilité\n",
        "!wget https://conservancy.umn.edu/bitstream/handle/11299/214865/dataset.zip?sequence=12&isAllowed=y\n",
        "!unzip /content/dataset.zip?sequence=12"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuMoQmEAVwfs"
      },
      "source": [
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6-SecLQN6hP"
      },
      "source": [
        "# The directory containing the source images\n",
        "data_path = \"dataset/instance_version/train\"\n",
        "\n",
        "# The path to the COCO labels JSON file\n",
        "labels_path = \"dataset/instance_version/instances_train_trashcan.json\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Version 4 - resize des images, en ne gardant que les bbox et category_id des targets normalisées, dans un array de dictionnaires (targets de taille variables)"
      ],
      "metadata": {
        "id": "23FFr0UOSPpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Attention \"bbox\": [x,y,width,height]\n",
        "class CocoDetection_diy_bis(data.Dataset) :\n",
        "    \"\"\"`MS Coco Detection <http://mscoco.org/dataset/#detections-challenge2016>`_ Dataset.\n",
        "\n",
        "    Args:\n",
        "        root (string): Root directory where images are downloaded to.\n",
        "        annFile (string): Path to json annotation file.\n",
        "        resize : (int,int) size of the images wanted \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root, annFile, size):\n",
        "        from pycocotools.coco import COCO\n",
        "        self.root = root\n",
        "        self.coco = COCO(annFile)\n",
        "        self.ids = list(self.coco.imgs.keys())\n",
        "        self.size = size\n",
        "        self.transform = transforms.Compose([transforms.Resize(size), transforms.ToTensor()])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "\n",
        "        Returns:\n",
        "            tuple: Tuple (image, target). target is the object returned by ``coco.loadAnns``.\n",
        "        \"\"\"\n",
        "        coco = self.coco\n",
        "        img_id = self.ids[index]\n",
        "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
        "        target = coco.loadAnns(ann_ids)\n",
        "\n",
        "        path = coco.loadImgs(img_id)[0]['file_name']\n",
        "\n",
        "        # Resize des images :\n",
        "        img = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
        "        original_size = img.size\n",
        "        img = self.transform(img)\n",
        "\n",
        "        # Targets dict :\n",
        "        targets = {'labels':[],'boxes':[]}\n",
        "\n",
        "        for elem in target :  \n",
        "          box = elem['bbox']\n",
        "          box[0] *= self.size[0] / original_size[0]\n",
        "          box[1] *= self.size[1] / original_size[1]\n",
        "          box[2] *= self.size[0] /original_size[0]\n",
        "          box[3] *= self.size[1] /  original_size[1]\n",
        "          targets['boxes'].append(box)\n",
        "          targets['labels'].append(elem['category_id'])\n",
        "\n",
        "        return img, targets\n",
        "        \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __repr__(self):\n",
        "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
        "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
        "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
        "        tmp = '    Transforms (if any): '\n",
        "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
        "        tmp = '    Target Transforms (if any): '\n",
        "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
        "        return fmt_str"
      ],
      "metadata": {
        "id": "oRookaleStv6"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn_diy (batch) : \n",
        "    \"\"\"\n",
        "    Parameters : \n",
        "    -----------\n",
        "    batch : list of tuples (img,targets)\n",
        "\n",
        "    Return : \n",
        "    -------\n",
        "    images : tensor of dim batch_size x 3 x 224 x 224\n",
        "    targets : list of dict containing : \n",
        "        - \"labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of ground-truth objects in the target) containing the class labels\n",
        "        - \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates\n",
        "    \"\"\"\n",
        "    imgs, trgts = list(zip(*batch)) # imgs et trgts sont désormais des batch_size-tuples \n",
        "\n",
        "    imgs = [img.unsqueeze(0) for img in list(imgs)] #ajout d'une dimension supplémentaire à tous les tenseurs\n",
        "    images = torch.cat(imgs) # concaténation en un seul tenseur\n",
        "\n",
        "    targets = []\n",
        "    for t in list(trgts) : \n",
        "      targets.append({'labels' : torch.from_numpy(np.array(t[\"labels\"])), \n",
        "                      'boxes' : torch.from_numpy(np.array(t[\"boxes\"]))})\n",
        "    \n",
        "    return images, targets"
      ],
      "metadata": {
        "id": "_4TouQrSSfmZ"
      },
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instances_train_4 = CocoDetection_diy_bis(root = data_path, annFile = labels_path, size=(224,224))\n",
        "# Format DataLoader :\n",
        "instances_train_dataloader_4 = DataLoader(instances_train_4, batch_size=1, shuffle=True, collate_fn = collate_fn_diy)\n",
        "test_img_dataload = next(iter(instances_train_dataloader_4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YF6RDy_eSfd9",
        "outputId": "59afcc64-7c36-4884-d700-7d143c5feb9d"
      },
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.28s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "\n",
        "resnet50 = models.resnet50(pretrained=True)\n",
        "image_torch = 800*torch.rand((1,3,800,800))\n",
        "#We choose the place where we extracted the feature map in order to get H_feature * W_feature around 2400 (papers)\n",
        "resnet50_features = nn.Sequential(*(list(resnet50.children())[:-5]))"
      ],
      "metadata": {
        "id": "JrGlsRp6tMVR"
      },
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_torch, dict_label = test_img_dataload\n",
        "gt_box = list(np.array(dict_label[0]['boxes']))\n",
        "gt_box_good_format = [xywh_to_vertice(element) for element in gt_box]\n",
        "ratio = [0.5,1,2]\n",
        "anchor_scales = [2,4,8]"
      ],
      "metadata": {
        "id": "HDok07qo1cRN"
      },
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = batch_training_proposal_RPN(image_torch, feature_shape, ratio, anchor_scales, gt_box_good_format,256,0.5,0.7,0.3)"
      ],
      "metadata": {
        "id": "Ht0IYlmmGf4z"
      },
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ah1baurMb_7n"
      },
      "source": [
        "# Training RPN network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:13:37.687322Z",
          "start_time": "2021-12-08T23:13:37.348201Z"
        },
        "id": "6hNzYE92b_7o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dce3a0b8-8bab-4f7c-8ce3-472d209e607c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 256, 200, 200])"
            ]
          },
          "metadata": {},
          "execution_count": 245
        }
      ],
      "source": [
        "from torchvision import models\n",
        "\n",
        "resnet50 = models.resnet50(pretrained=True)\n",
        "image_torch = 800*torch.rand((1,3,800,800))\n",
        "#We choose the place where we extracted the feature map in order to get H_feature * W_feature around 2400 (papers)\n",
        "resnet50_features = nn.Sequential(*(list(resnet50.children())[:-5]))\n",
        "resnet50_features(image_torch).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm6g59e7b_7p"
      },
      "source": [
        "https://stackoverflow.com/questions/69480764/what-is-the-difference-between-resnet50-vgg16-etc-and-rcnn-faster-rcnn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 290,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:13:41.640996Z",
          "start_time": "2021-12-08T23:13:41.621664Z"
        },
        "code_folding": [],
        "id": "EYl81Phib_7p"
      },
      "outputs": [],
      "source": [
        "class RPN(nn.Module):\n",
        "    #TODO : remove embedding_dim using wv.shape[1]\n",
        "    #define all the layers used in model\n",
        "    def __init__(self,mid_channels, in_channels,nb_anchors,pre_trained_model):\n",
        "        \n",
        "        #Constructor\n",
        "        super().__init__()        \n",
        "        \n",
        "        #embedding layer\n",
        "        self.pre_trained_model = pre_trained_model\n",
        "        self.conv1 = nn.Conv2d(in_channels, mid_channels, 3, 1, 1)\n",
        "        self.conv1.weight.data.normal_(0, 0.01)\n",
        "        self.conv1.bias.data.zero_()\n",
        "        self.sm = nn.Softmax(dim = 2)\n",
        "        self.reg_layer = nn.Conv2d(mid_channels, nb_anchors *4, 1, 1, 0)\n",
        "        self.reg_layer.weight.data.normal_(0, 0.01)\n",
        "        self.reg_layer.bias.data.zero_()\n",
        "\n",
        "        self.cls_layer = nn.Conv2d(mid_channels, nb_anchors *2, 1, 1, 0)\n",
        "        # classification layer\n",
        "        self.cls_layer.weight.data.normal_(0, 0.01)\n",
        "        self.cls_layer.bias.data.zero_()\n",
        "       \n",
        "\n",
        "    def forward(self,img):\n",
        "        x = self.pre_trained_model(img)\n",
        "        x = self.conv1(x)\n",
        "        pred_anchor = self.reg_layer(x)\n",
        "        pred_anchor = pred_anchor.permute(0, 2, 3, 1).contiguous().view(1, -1, 4)\n",
        "        \n",
        "        pred_cls = self.cls_layer(x)\n",
        "        pred_cls = pred_cls.permute(0, 2, 3, 1).contiguous()\n",
        "        pred_cls = pred_cls.view(1, -1, 2)\n",
        "        pred_cls = self.sm(pred_cls)\n",
        "        return pred_anchor, pred_cls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 293,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:13:52.294384Z",
          "start_time": "2021-12-08T23:13:52.196399Z"
        },
        "scrolled": true,
        "id": "lzsPxisgb_7p",
        "outputId": "2c9523ca-c7ae-46a2-c010-cd32e1e295d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28224, 4])\n",
            "torch.Size([1, 28224, 2])\n"
          ]
        }
      ],
      "source": [
        "model = RPN(256,256,9,resnet50_features)\n",
        "image_torch = 800*torch.rand((1,3,224,224)) #scale picture ? mean ? \n",
        "res = model(image_torch)\n",
        "\n",
        "print(res[0].shape)\n",
        "print(res[1].shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model(res['image'])\n",
        "pred_box, pred_cls = model(res['image'])\n",
        "print(pred_box.shape)\n",
        "print(pred_cls.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmaUWQPAh0Kr",
        "outputId": "3d472e88-fd55-4e46-bb4a-27558b1f24f9"
      },
      "execution_count": 275,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28224, 4])\n",
            "torch.Size([1, 28224, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res"
      ],
      "metadata": {
        "id": "Np2ZWuw_lffk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_box_pos_neut = torch.index_select(pred_box, 1, torch.from_numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kq5n6O3yjZcT",
        "outputId": "b4addd5c-c8a6-4923-a228-aa8c7ad387a1"
      },
      "execution_count": 265,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 256, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 265
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ind_pos_neu = res[\"indices\"]\n",
        "ind_pos = ind_pos_neu[np.where((res[\"labels\"] == 1))[0]]\n",
        "\n",
        "ind_pos_neu = torch.from_numpy(ind_pos_neu)\n",
        "ind_pos = torch.from_numpy(ind_pos)\n",
        "\n",
        "pred_box_pn = torch.index_select(pred_box, 1, ind_pos_neu)\n",
        "pred_box_p = torch.index_select(pred_box, 1, ind_pos)\n",
        "\n",
        "pred_cls_pn = torch.index_select(pred_cls, 1, ind_pos_neu)\n",
        "pred_cls_p = torch.index_select(pred_cls, 1, ind_pos)"
      ],
      "metadata": {
        "id": "qZZQZSknlqwB"
      },
      "execution_count": 271,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def box_for_loss(real, predicted, only_pos = False):\n",
        "  if(only_pos):\n",
        "    ind = real[\"indices\"]\n",
        "  else:\n",
        "    ind = (real[\"indices\"])[np.where((real[\"labels\"] == 1))[0]]\n",
        "  \n",
        "  ind = torch.from_numpy(ind)\n",
        "\n",
        "  pred_box = torch.index_select(predicted[0], 1, ind)\n",
        "  pred_cls = torch.index_select(predicted[1], 1, ind)\n",
        "\n",
        "  return({\"real_box\" : real[\"boxes\"], \"real_cls\" : real[\"labels\"], \"pred_box\" : pred_box, \"pred_cls\" : pred_cls})"
      ],
      "metadata": {
        "id": "Tc9aHYvhm7_K"
      },
      "execution_count": 282,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "box_for_loss(res, pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5SRS0o-ooWT",
        "outputId": "afc7ec7f-c064-4c08-a7f0-00fa699d04ae"
      },
      "execution_count": 283,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'pred_box': tensor([[[-2.8207e-03, -1.0063e-02, -4.5098e-03,  2.9165e-02],\n",
              "          [-2.6256e-02,  1.8114e-02,  2.3893e-02, -3.6274e-03],\n",
              "          [-1.7059e-02,  2.2413e-02,  4.1013e-02,  5.8104e-02],\n",
              "          [-1.0880e-02,  2.0852e-02,  2.6431e-02,  3.6812e-02],\n",
              "          [ 3.4777e-03,  1.0685e-02, -2.5200e-02,  3.4356e-02],\n",
              "          [-3.2425e-02, -1.2288e-02, -2.8283e-02,  2.1855e-02],\n",
              "          [-1.4770e-02, -2.3048e-02,  5.8565e-03, -8.3016e-03],\n",
              "          [-1.6058e-02, -1.3638e-02,  1.5984e-02,  2.6563e-02],\n",
              "          [-2.5865e-02, -2.4932e-02, -2.0518e-02,  1.2569e-04],\n",
              "          [ 6.9739e-03, -9.2644e-03, -7.9378e-03,  1.8616e-02],\n",
              "          [ 3.6901e-03, -1.1572e-02,  2.3516e-02,  2.7848e-02],\n",
              "          [-4.9335e-03, -9.2937e-03,  7.6302e-03,  5.3801e-02],\n",
              "          [ 1.1247e-02, -2.4447e-02, -9.8188e-03,  2.2281e-02],\n",
              "          [-3.2328e-02,  9.7744e-03,  2.8902e-03,  3.0646e-02],\n",
              "          [-1.0184e-02,  1.4560e-02, -9.5751e-03,  3.1511e-02],\n",
              "          [-1.5839e-02,  8.0279e-04,  1.2685e-02,  6.9807e-03],\n",
              "          [-2.8519e-02,  6.7979e-03, -5.5571e-03,  1.5745e-02],\n",
              "          [-3.3627e-02,  1.1653e-02,  1.5601e-02,  3.7560e-02],\n",
              "          [-6.5567e-02,  3.6790e-02,  1.8641e-02,  2.2916e-02],\n",
              "          [-2.7638e-02,  1.5698e-02,  1.2435e-02, -1.1833e-02],\n",
              "          [-9.8046e-03,  9.5604e-03,  2.2321e-02,  1.5988e-02],\n",
              "          [-8.8288e-03,  4.5219e-02, -2.3167e-02,  4.0694e-02],\n",
              "          [-2.4588e-02, -1.3689e-02,  1.3571e-02,  6.9281e-02],\n",
              "          [-4.3073e-02, -1.9375e-02,  3.1353e-02,  9.0626e-04],\n",
              "          [ 5.9233e-03, -1.7942e-02, -1.0129e-02,  2.5781e-02],\n",
              "          [-2.9450e-02,  4.0220e-04,  3.3719e-02,  3.7043e-02],\n",
              "          [-4.1984e-02,  7.4356e-03,  3.1859e-02,  2.7570e-02],\n",
              "          [-2.0905e-02, -1.4855e-03,  9.4659e-03,  1.7830e-02],\n",
              "          [-5.7745e-03, -1.3128e-02, -1.0200e-02,  4.6299e-02],\n",
              "          [-8.4516e-03,  3.6957e-02, -2.8695e-02,  3.8053e-02],\n",
              "          [-2.5812e-02, -4.3002e-02, -2.8269e-03,  1.1176e-02],\n",
              "          [-2.8001e-02,  5.8452e-04, -1.4644e-02,  2.4633e-02],\n",
              "          [-2.9774e-02, -6.0907e-04,  4.6306e-02,  5.1339e-02],\n",
              "          [ 7.3089e-03, -6.7183e-03,  3.6279e-02,  3.2922e-02],\n",
              "          [-4.2266e-02, -6.9610e-03,  1.8475e-03,  2.4203e-02],\n",
              "          [ 2.6372e-03,  1.9753e-02,  1.1207e-02,  3.8313e-02],\n",
              "          [-1.0776e-03,  2.3239e-02, -3.4343e-02,  4.4119e-02],\n",
              "          [-2.7353e-02, -2.4917e-03,  3.7043e-02,  3.6227e-02],\n",
              "          [-2.0381e-02, -2.0988e-02,  2.3429e-02,  1.7590e-02],\n",
              "          [-7.3697e-03, -1.5210e-02,  1.0993e-02,  3.1041e-02],\n",
              "          [-2.4710e-02, -6.6687e-04, -1.1328e-02,  3.1041e-02],\n",
              "          [-3.3381e-02,  1.3183e-02,  4.2435e-04,  2.1419e-02],\n",
              "          [-3.1620e-02, -9.2465e-03, -3.9211e-05,  4.8567e-02],\n",
              "          [-4.0096e-02, -1.4267e-02, -1.6590e-03,  1.5376e-02],\n",
              "          [-4.2989e-02, -1.2533e-02,  2.5118e-02,  6.7015e-03],\n",
              "          [ 3.6357e-03,  1.1191e-02,  2.6060e-02,  2.7369e-02],\n",
              "          [-1.2038e-02, -1.0533e-02, -1.8501e-02,  4.2374e-02],\n",
              "          [-1.2795e-02,  7.0787e-03,  1.4403e-02,  2.7694e-02],\n",
              "          [-4.6142e-02, -1.7902e-02,  2.1690e-02,  2.8683e-02],\n",
              "          [-4.4344e-02, -6.7677e-03,  1.5731e-02,  2.3545e-02],\n",
              "          [-9.7504e-03, -1.2549e-02,  2.5129e-02,  5.4851e-02],\n",
              "          [-2.9476e-02, -1.9424e-02,  3.2324e-02,  1.1270e-02],\n",
              "          [-2.1937e-02,  9.1217e-03,  6.5538e-03,  3.9758e-02],\n",
              "          [-2.6705e-02,  2.1927e-02,  2.0074e-02,  4.7503e-02],\n",
              "          [-6.8731e-03,  2.1992e-02, -3.6775e-03,  6.2883e-02],\n",
              "          [ 4.5005e-03, -6.0114e-03,  2.7208e-03,  6.7499e-02],\n",
              "          [ 3.8621e-03,  1.3354e-02,  9.7390e-03,  2.9109e-02],\n",
              "          [-6.0859e-03,  2.3740e-02,  8.6617e-03,  3.8079e-02],\n",
              "          [ 1.5722e-03,  2.6974e-02,  2.3681e-02,  5.8252e-02],\n",
              "          [-1.9883e-02,  4.2147e-03,  1.7924e-02,  2.6289e-02],\n",
              "          [-1.5321e-02, -5.2008e-03,  2.1000e-02,  2.1781e-02],\n",
              "          [ 2.3694e-02,  2.6577e-02,  1.5073e-02,  2.8077e-02],\n",
              "          [ 1.7863e-02, -1.1566e-02, -9.6208e-03,  4.4156e-02],\n",
              "          [-1.3215e-02, -9.0562e-03,  9.0249e-03,  3.4312e-02],\n",
              "          [ 4.1301e-03, -6.6708e-04,  8.7386e-03,  2.6311e-02],\n",
              "          [ 2.7110e-02,  2.1019e-02, -9.2993e-03,  3.2236e-02],\n",
              "          [ 8.5976e-03, -1.5283e-02, -1.2520e-02,  2.5868e-02],\n",
              "          [-3.8074e-02,  2.0741e-02,  1.7926e-02,  2.8477e-02],\n",
              "          [-1.6432e-03, -2.8672e-02, -6.6850e-05, -4.3881e-03],\n",
              "          [-2.8768e-02, -4.0821e-02, -3.2038e-02, -9.2382e-03],\n",
              "          [-7.8871e-03, -1.3215e-02,  9.4532e-03,  2.7122e-02],\n",
              "          [-1.6603e-02, -1.1770e-02, -1.7608e-02,  3.1711e-02],\n",
              "          [-2.0031e-02,  2.7253e-03, -7.2588e-03,  2.2074e-02],\n",
              "          [-1.9713e-02, -1.1435e-02, -1.1471e-02,  2.4572e-02],\n",
              "          [-1.6357e-02,  1.8389e-02,  9.4580e-03,  3.2169e-02],\n",
              "          [-3.8625e-02, -1.1151e-02, -3.6959e-03,  4.6151e-02],\n",
              "          [-1.4805e-02, -1.2855e-02,  2.3485e-02,  3.1949e-02],\n",
              "          [-5.0972e-03, -1.5891e-02,  1.0584e-04,  2.7266e-02],\n",
              "          [-9.1674e-03, -2.9021e-03,  8.0807e-03,  1.5733e-02],\n",
              "          [-1.1499e-02, -2.4805e-02,  2.2076e-02,  2.7429e-02],\n",
              "          [-1.3844e-02,  4.7358e-03, -1.0951e-02,  2.0603e-02],\n",
              "          [ 2.9353e-03, -8.5600e-03,  2.9929e-02,  4.1698e-02],\n",
              "          [-1.8447e-02, -1.2442e-02,  2.0553e-03,  1.2310e-02],\n",
              "          [ 2.6855e-02,  9.9726e-03, -1.6158e-02,  2.7249e-02],\n",
              "          [ 4.1667e-02,  1.4887e-02, -1.3100e-02, -1.0562e-02],\n",
              "          [ 3.4840e-02,  2.4243e-02,  1.0848e-02, -1.5283e-03],\n",
              "          [ 2.0153e-02,  4.0426e-03,  2.0372e-02,  4.4229e-02],\n",
              "          [ 2.1948e-02, -2.0888e-03,  4.2177e-02,  3.3673e-02],\n",
              "          [-6.1918e-03, -2.2283e-02,  2.4601e-02,  6.0648e-02],\n",
              "          [ 6.8224e-03, -1.5183e-04,  2.9848e-02,  1.9050e-02],\n",
              "          [ 2.2957e-03,  1.1145e-02,  2.5681e-02,  2.4201e-02],\n",
              "          [ 2.8644e-02, -4.6368e-02,  8.6312e-03,  2.8021e-02],\n",
              "          [ 3.8608e-02,  1.2378e-02,  1.4012e-02,  5.7300e-03],\n",
              "          [-1.2730e-02, -1.7769e-02, -3.5396e-02,  1.3557e-02],\n",
              "          [ 5.7558e-03,  1.8262e-02, -3.0035e-02,  2.3602e-02],\n",
              "          [ 3.7766e-03, -2.3553e-02,  2.0173e-02,  3.5213e-02],\n",
              "          [ 5.5924e-02,  5.2052e-02, -1.0198e-02,  1.5932e-04],\n",
              "          [-1.9043e-02, -7.2081e-03,  1.4295e-02,  3.8373e-02],\n",
              "          [ 1.6304e-02,  3.8124e-02, -2.2231e-03,  2.0758e-02],\n",
              "          [ 1.4030e-02,  1.6264e-02, -4.4223e-03,  1.2203e-02],\n",
              "          [ 2.7318e-02,  1.0233e-02, -1.5760e-02, -3.3281e-03],\n",
              "          [ 9.2504e-03,  8.4089e-03,  1.0840e-03,  1.7229e-02],\n",
              "          [ 1.8047e-02,  1.9990e-02, -4.0511e-05,  4.4166e-02],\n",
              "          [ 2.7568e-02,  4.5834e-02, -3.5506e-02,  4.5023e-02],\n",
              "          [-3.8024e-02,  3.2008e-03,  3.0641e-02,  1.7895e-02],\n",
              "          [-9.2865e-03, -1.0518e-02, -1.5947e-03,  3.6752e-02],\n",
              "          [-9.7841e-03, -5.8829e-03, -1.2437e-02,  1.7816e-02],\n",
              "          [ 5.1032e-02,  4.5920e-02,  1.7506e-03,  2.1109e-02],\n",
              "          [-3.5428e-03,  2.1825e-02,  5.8196e-03,  5.2009e-02],\n",
              "          [-4.4644e-03,  3.3759e-03, -1.8357e-02,  1.0503e-02],\n",
              "          [ 5.0678e-02,  1.0915e-02, -7.2590e-03, -1.2242e-02],\n",
              "          [ 2.4419e-02,  1.5716e-03, -1.4157e-02,  1.3700e-02],\n",
              "          [-3.5435e-02, -5.2251e-03,  1.3560e-03,  2.2838e-02],\n",
              "          [-1.2618e-02, -3.0911e-02,  1.7287e-02,  1.5974e-02],\n",
              "          [-3.9943e-02, -6.3363e-03, -4.6117e-03,  1.3174e-02],\n",
              "          [-3.4081e-03,  5.8997e-03, -2.5238e-02,  1.6388e-02],\n",
              "          [-1.5830e-02,  3.4493e-04,  1.3270e-02,  1.0994e-02],\n",
              "          [-2.7941e-02,  1.0353e-02, -9.0133e-03,  3.5648e-02],\n",
              "          [-9.4596e-03, -1.3401e-02, -4.6258e-03,  3.8478e-02],\n",
              "          [-3.1729e-02,  7.2676e-03,  1.1422e-02,  2.0908e-02],\n",
              "          [-3.2746e-02, -3.8305e-02, -5.9915e-03,  3.7220e-02],\n",
              "          [-1.2710e-02, -8.8776e-03,  2.2033e-02,  1.6941e-02],\n",
              "          [ 1.0601e-02, -1.0284e-02, -9.7914e-03,  1.2791e-02],\n",
              "          [-3.3463e-02, -1.3993e-02,  1.0712e-02,  2.9394e-02],\n",
              "          [-3.5031e-02,  1.6487e-02,  8.0526e-04,  3.6395e-02],\n",
              "          [-1.4456e-02,  1.6608e-02,  1.1706e-02,  3.2356e-02],\n",
              "          [-1.0015e-02, -4.9728e-03,  3.4224e-02,  1.6020e-02],\n",
              "          [-1.0739e-02,  3.3611e-04,  2.2503e-03,  1.2306e-02]]],\n",
              "        grad_fn=<IndexSelectBackward0>),\n",
              " 'pred_cls': tensor([[[ 0.0210,  0.0048],\n",
              "          [ 0.0177,  0.0155],\n",
              "          [ 0.0058, -0.0200],\n",
              "          [ 0.0006, -0.0360],\n",
              "          [-0.0244,  0.0197],\n",
              "          [-0.0130,  0.0225],\n",
              "          [ 0.0283, -0.0047],\n",
              "          [ 0.0101,  0.0280],\n",
              "          [-0.0033,  0.0374],\n",
              "          [ 0.0205,  0.0264],\n",
              "          [-0.0040,  0.0180],\n",
              "          [-0.0123, -0.0183],\n",
              "          [-0.0221, -0.0083],\n",
              "          [ 0.0051,  0.0237],\n",
              "          [-0.0066, -0.0022],\n",
              "          [-0.0095,  0.0092],\n",
              "          [-0.0023,  0.0054],\n",
              "          [-0.0263,  0.0369],\n",
              "          [-0.0194,  0.0403],\n",
              "          [ 0.0035,  0.0017],\n",
              "          [-0.0184, -0.0196],\n",
              "          [ 0.0102,  0.0317],\n",
              "          [-0.0055,  0.0330],\n",
              "          [ 0.0005,  0.0123],\n",
              "          [-0.0067,  0.0195],\n",
              "          [-0.0102,  0.0126],\n",
              "          [-0.0331, -0.0087],\n",
              "          [-0.0050,  0.0042],\n",
              "          [ 0.0159,  0.0157],\n",
              "          [-0.0010,  0.0196],\n",
              "          [-0.0041,  0.0024],\n",
              "          [ 0.0247,  0.0241],\n",
              "          [-0.0039, -0.0194],\n",
              "          [ 0.0013,  0.0183],\n",
              "          [ 0.0220,  0.0172],\n",
              "          [-0.0199,  0.0137],\n",
              "          [-0.0057, -0.0087],\n",
              "          [-0.0015, -0.0068],\n",
              "          [ 0.0174,  0.0385],\n",
              "          [-0.0120, -0.0114],\n",
              "          [-0.0065,  0.0140],\n",
              "          [ 0.0067,  0.0116],\n",
              "          [-0.0043,  0.0130],\n",
              "          [-0.0173,  0.0336],\n",
              "          [-0.0045,  0.0263],\n",
              "          [ 0.0155, -0.0030],\n",
              "          [-0.0101,  0.0192],\n",
              "          [-0.0279,  0.0177],\n",
              "          [-0.0016,  0.0220],\n",
              "          [ 0.0095,  0.0100],\n",
              "          [ 0.0025,  0.0267],\n",
              "          [-0.0115,  0.0265],\n",
              "          [-0.0122,  0.0333],\n",
              "          [-0.0134,  0.0276],\n",
              "          [ 0.0072, -0.0122],\n",
              "          [-0.0126,  0.0358],\n",
              "          [ 0.0421,  0.0213],\n",
              "          [ 0.0037, -0.0071],\n",
              "          [ 0.0049,  0.0235],\n",
              "          [-0.0084,  0.0061],\n",
              "          [ 0.0018,  0.0229],\n",
              "          [ 0.0182,  0.0330],\n",
              "          [ 0.0124,  0.0255],\n",
              "          [ 0.0220,  0.0175],\n",
              "          [ 0.0040,  0.0114],\n",
              "          [-0.0014,  0.0473],\n",
              "          [ 0.0049,  0.0323],\n",
              "          [-0.0083,  0.0237],\n",
              "          [-0.0413,  0.0416],\n",
              "          [-0.0190,  0.0379],\n",
              "          [ 0.0040,  0.0052],\n",
              "          [ 0.0015,  0.0335],\n",
              "          [ 0.0095,  0.0425],\n",
              "          [-0.0044,  0.0104],\n",
              "          [-0.0125,  0.0127],\n",
              "          [-0.0246,  0.0011],\n",
              "          [ 0.0090,  0.0285],\n",
              "          [ 0.0023,  0.0239],\n",
              "          [-0.0136,  0.0389],\n",
              "          [-0.0145,  0.0431],\n",
              "          [ 0.0106,  0.0041],\n",
              "          [-0.0140,  0.0027],\n",
              "          [-0.0177,  0.0508],\n",
              "          [ 0.0198,  0.0334],\n",
              "          [-0.0361,  0.0097],\n",
              "          [-0.0038,  0.0102],\n",
              "          [-0.0041, -0.0203],\n",
              "          [-0.0089,  0.0377],\n",
              "          [-0.0073,  0.0029],\n",
              "          [-0.0243,  0.0486],\n",
              "          [-0.0036,  0.0152],\n",
              "          [ 0.0073,  0.0504],\n",
              "          [ 0.0117,  0.0136],\n",
              "          [-0.0155, -0.0093],\n",
              "          [ 0.0125,  0.0295],\n",
              "          [-0.0254,  0.0164],\n",
              "          [-0.0015, -0.0030],\n",
              "          [-0.0318,  0.0110],\n",
              "          [-0.0074,  0.0227],\n",
              "          [-0.0193,  0.0478],\n",
              "          [-0.0137,  0.0221],\n",
              "          [ 0.0010,  0.0323],\n",
              "          [ 0.0033,  0.0468],\n",
              "          [-0.0223,  0.0358],\n",
              "          [ 0.0030,  0.0244],\n",
              "          [ 0.0023,  0.0212],\n",
              "          [ 0.0123, -0.0169],\n",
              "          [ 0.0009,  0.0173],\n",
              "          [ 0.0172,  0.0241],\n",
              "          [-0.0138,  0.0330],\n",
              "          [-0.0290,  0.0425],\n",
              "          [-0.0291,  0.0429],\n",
              "          [-0.0016,  0.0106],\n",
              "          [-0.0051,  0.0378],\n",
              "          [-0.0039,  0.0127],\n",
              "          [ 0.0125,  0.0298],\n",
              "          [-0.0048,  0.0050],\n",
              "          [ 0.0064,  0.0014],\n",
              "          [-0.0066,  0.0102],\n",
              "          [-0.0087,  0.0101],\n",
              "          [ 0.0282,  0.0126],\n",
              "          [ 0.0031,  0.0064],\n",
              "          [-0.0040,  0.0041],\n",
              "          [ 0.0078,  0.0178],\n",
              "          [-0.0067,  0.0234],\n",
              "          [ 0.0050,  0.0032],\n",
              "          [ 0.0045, -0.0013],\n",
              "          [-0.0197,  0.0133]]], grad_fn=<IndexSelectBackward0>),\n",
              " 'real_box': tensor([[ 6.6598,  9.8183,  1.2678,  2.1259],\n",
              "         [11.7060, 13.4333,  2.2330,  2.4394],\n",
              "         [ 9.4838, 13.4333,  2.2330,  2.4394],\n",
              "         ...,\n",
              "         [-0.2302, -3.3478,  0.5960,  1.4742],\n",
              "         [ 1.0208, -4.8882,  1.5970,  1.8034],\n",
              "         [ 2.4274, -7.0734,  1.2678,  2.1259]], dtype=torch.float64),\n",
              " 'real_cls': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
              "         1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
              "         0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
              "         0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n",
              "         0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,\n",
              "         0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
              "         1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
            ]
          },
          "metadata": {},
          "execution_count": 283
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rpn_loss(real, predicted):\n",
        "  "
      ],
      "metadata": {
        "id": "BMd7HMmjmS97"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "stat_app",
      "language": "python",
      "name": "stat_app"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "anchor_boxes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}